### 2017-11
- Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training, Anonymous, 2017. [openreview](https://openreview.net/pdf?id=SkhQHMW0W)
- Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling, Inan et al 2017. [arXiv](https://arxiv.org/abs/1611.01462)
- Building machines that learn and think for themselves, Botvinick et al, 2017. [cambridge](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-for-themselves/E28DBFEC380D4189FB7754B50066A96F)
- Neural Discrete Representation Learning, va den Oord et al, 2017. [arXiv](https://arxiv.org/abs/1711.00937)
- Evolution Strategies, Otoro 2017, blog part [1](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/), [2](http://blog.otoro.net/2017/11/12/evolving-stable-strategies/)
- Matrix Capsules with EM Routing. Anonymous (likely Hinton lab), 2017. [openreview](https://openreview.net/pdf?id=HJWLfGWRb).
- Dynamic Routing Between Capsules, Sabour et al, 2017. [arXiv](https://arxiv.org/abs/1710.09829). [`code-keras`](https://github.com/XifengGuo/CapsNet-Keras)
- Weighted Transformer Network for Machine Translation, Ahmed et al, 2017. [arXiv](https://arxiv.org/abs/1711.02132)
- Unsupervised Machine Translation Using Monolingual Corpora Only, Lample et al, 2017. [arXiv](https://arxiv.org/abs/1711.00043)
- Non-Autoregressive Neural Machine Translation, Gu et al, 2017. [arXiv](https://arxiv.org/abs/1711.02281)
- Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments, Lowe et al, 2017. [arXiv](https://arxiv.org/abs/1706.02275)

### 2017-10
- Adversarial Learning for Neural Dialogue Generation, Li et al, 2017. [arXiv](https://arxiv.org/abs/1701.06547)
- Frustratingly Short Attention Spans in Neural Language Modeling, Daniluk et al, 2017. [arXiv](https://arxiv.org/abs/1702.04521)
- Adversarial Training Methods for Semi-Supervised Text Classification, Miyato et al, 2017. [arXiv](https://arxiv.org/abs/1605.07725)
- Progressive Growing of GANs for Improved Quality, Stability, and Variation, Karras et al, 2017. [pdf](http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of//karras2017gan-paper.pdf)
- A Closer Look at Memorization in Deep Networks, Arpit et al, 2017. [arXiv](https://arxiv.org/abs/1706.05394)
- Understanding deep learning requires rethinking generalization, Zhang et al, 2016. [arXiv](https://arxiv.org/abs/1611.03530)
- The Loss Surfaces of Multilayer Networks, Choromanska et al, 2015. [arXiv](https://arxiv.org/abs/1412.0233)
- Meta Learning Shared Hierarchies, Frans et al, 2017. [arXiv](https://arxiv.org/abs/1710.09767), [`author blog`](https://blog.openai.com/learning-a-hierarchy/)
- Mastering the game of Go without human knowledge, Silver et al, 2017. [arXiv](https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html), [`blog`](http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/)
- Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation, Sharma et al, 2017. [arXiv](https://arxiv.org/abs/1706.09799)
- GuessWhat?! Visual object discovery through multi-modal dialogue, de Vries et al, 2017. [arXiv](https://arxiv.org/abs/1611.08481)
- A Frame Tracking Model for Memory-Enhanced Dialogue Systems, Schulz et al, 2017. [arXiv](https://arxiv.org/abs/1706.01690)
- A Deep Reinforced Model for Abstractive Summarization, Paulus et al, 2017. [arXiv](https://arxiv.org/abs/1705.04304), [`author blog`](https://einstein.ai/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)
- (about ROUGE score for summarization) ROUGE: A Package for Automatic Evaluation of Summaries, Chin-Yew Lin, 2004. [acl](http://anthology.aclweb.org/W/W04/W04-1013.pdf)
- Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al, 2017. [arXiv](https://arxiv.org/abs/1710.02298)
- Language Modeling with Gated Convolutional Networks, Dauphin et al, 2017, [arXiv](https://arxiv.org/abs/1612.08083)
- Convolutional Sequence to Sequence Learning, Gehring et al, 2017. [arXiv](https://arxiv.org/abs/1705.03122)
- Emergence of Grounded Compositional Language in Multi-Agent Populations, Mordatch and Abbeel, 2017. [arXiv](https://arxiv.org/abs/1703.04908), [`author blog`](https://blog.openai.com/learning-to-communicate/). Note: related to Kottur et al 2017.

### 2017-09
- Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog, Kottur et al, 2017. [arXiv](https://arxiv.org/abs/1706.08502)
- Opening the black box of Deep Neural Networks via Information, Schwartz-Ziv and Tishbly, 2017. [arXiv](https://arxiv.org/abs/1703.00810), [m-p review](https://blog.acolyer.org/2017/11/15/opening-the-black-box-of-deep-neural-networks-via-information-part-i/)
- End-to-end Neural Coreference Resolution, Lee et al, 2017. [arXiv](https://arxiv.org/abs/1707.07045)
- Deep Reinforcement Learning for Mention-Ranking Coreference Models, Clark et al, 2016. [arXiv](https://arxiv.org/abs/1609.08667)
- Oriented Response Networks, Zhou et al 2017. [arXiv](https://arxiv.org/abs/1701.01833)
- Training RNNs as Fast as CNNs, Lei et al, 2017. [arXiv](https://arxiv.org/abs/1709.02755)
- Quasi-Recurrent Neural Networks, Bradbury et al 2017. [arXiv](https://openreview.net/pdf?id=H1zJ-v5xl), [`author blog/code`](https://einstein.ai/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding)
- A Deep Reinforcement Learning Chatbot, Serban et al, 2017. [arXiv](https://arxiv.org/abs/1709.02349)
- Independently Controllable Factors, Thomas et al, 2017. [arXiv](https://arxiv.org/abs/1708.01289)
- Attention Is All You Need, Vaswani et al, 2017. [arXiv](https://arxiv.org/abs/1706.03762), [`code`](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py), [reddit](https://www.reddit.com/r/MachineLearning/comments/6gwqiw/r_170603762_attention_is_all_you_need_sota_nmt/)
- Attention-over-Attention Neural Networks for Reading Comprehension, Cui et al 2017. [arXiv](https://arxiv.org/abs/1607.04423), [`code`](https://github.com/OlavHN/attention-over-attention)
- Get To The Point: Summarization with Pointer-Generator Networks, See et al 2017. [arXiv](https://arxiv.org/abs/1704.04368), [`author blog`](http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html), [`code`](https://github.com/abisee/cnn-dailymail)
- Î²-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK, Higgins et al 2017. [pdf](https://openreview.net/pdf?id=Sy2fzU9gl)
- Massive Exploration of Neural Machine Translation Architectures, Britz et al 2017. [arXiv](https://arxiv.org/abs/1703.03906v2)
- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017. [arXiv](https://arxiv.org/abs/1703.10593), ['examples'](https://junyanz.github.io/CycleGAN/), [`code-torch`](https://github.com/junyanz/CycleGAN), [`code-PyT`](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)

### 2017-08
- A Brief Survey of Deep Reinforcement Learning, Arulkumaran et al 2017. [arXiv](https://arxiv.org/abs/1708.05866)
- Regularizing and Optimizing LSTM Language Models, Merity et al 2017. [arXiv](http://lanl.arxiv.org/abs/1708.02182v1)
- Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets, Yang et al 2017. [arXiv](https://arxiv.org/abs/1703.04887)
- Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, Zhao et al 2017. [arXiv](https://arxiv.org/abs/1703.10960)
- How to Train Your DRAGAN, Kodali et al 2017. [arXiv](https://arxiv.org/abs/1705.07215)
- Improved Training of Wasserstein GANs, Gulrajani et al 2017. [arXiv](https://arxiv.org/abs/1704.00028), [`blog`](https://lernapparat.de/improved-wasserstein-gan/), [`blog`](http://lernapparat.de/more-improved-wgan/), [`code`](https://github.com/igul222/improved_wgan_training)
- Wasserstein Gan, Arjovsky et al 2017. [arXiv](https://arxiv.org/abs/1701.07875), [`read-through`](http://www.alexirpan.com/2017/02/22/wasserstein-gan.html), [`Kantorovich-Rubinstein duality`](https://vincentherrmann.github.io/blog/wasserstein/), [`WGAN-tensorflow`](https://github.com/shekkizh/WassersteinGAN.tensorflow), [`blog/code`](https://wiseodd.github.io/techblog/2017/02/04/wasserstein-gan/)
- Reading Scene Text in Deep Convolutional Sequences, He et al, 2016. [arXiv](https://arxiv.org/abs/1506.04395)

### 2017-03
- Recurrent Batch Normalization, Cooijmans et al, 2017. [arXiv](https://arxiv.org/abs/1603.09025), [`code-tf`](https://github.com/OlavHN/bnlstm)
- An Actor-Critic Algorithm for Sequence Prediction, Bahdanau et al 2017. [arXiv](https://arxiv.org/abs/1607.07086), [`code`](https://github.com/rizar/actor-critic-public)
- Scheduled Sampling for Sequence Prediction with RNN, Bengio et al, 2015 [arXiv](https://arxiv.org/abs/1506.03099), [`summary`](summaries/scheduled_sampling.md),
- Hybrid computing using a neural network with dynamic external memory, published in [Nature](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf)
- Neural Turing Machine, [arXiv](https://arxiv.org/abs/1410.5401)
- LEARNING END-TO-END GOAL-ORIENTED DIALOG, Bordes et al, 2017. [arXiv](https://arxiv.org/abs/1605.07683), [`code`](https://github.com/carpedm20/MemN2N-tensorflow)
- End-To-End Memory Networks, Sukhbaatar et al, 2015, [arXiv](https://arxiv.org/abs/1503.08895)
- Memory Networks, [arXiv](https://arxiv.org/abs/1410.3916)
- Deep Photo Style Transfer, [arXiv](https://arxiv.org/abs/1703.07511)
- Matching Networks for One Shot Learning, Vinyals et al, NIPS 2016. [arXiv](https://arxiv.org/abs/1606.04080). [`summary`](summaries/matching_networks.md), [`code`](https://github.com/zergylord/oneshot). [`karpathy notes`](http://www.shortscience.org/paper?bibtexKey=journals/corr/VinyalsBLKW16#karpathy), [`Colyer blog`](https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/)

### 2017-01
- Optimization As A Model For Few-Shot Learning, Sachin Ravi and Hugo Larochelle, ICLR 2017. [openreview](https://openreview.net/pdf?id=rJY0-Kcll), [video](https://www.youtube.com/watch?v=igJmB6d8y8E)
- NIPS 2016 Tutorial:Generative Adversarial Networks, [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOa2RqZmJVR2hrUTA/view?usp=sharing),[arXiv](https://arxiv.org/abs/1701.00160), [blog/code](https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/)

### 2016-11
- [Fully Character-Level Neural Machine Translation without Explicit Segmentation](summaries/fully_char_level_nmt.md), [annotated](https://drive.google.com/open?id=0ByV7wn2NzevOQ0JtTTRuR0pjUlE), [arXiv](https://arxiv.org/abs/1610.03017)
- [Neural Machine Translation by Jointly Learning to Align and Translate](summaries/neural_machine_translation.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOS3FmWHVNazhnczA/view?usp=sharing), [arXiv](https://arxiv.org/abs/1409.0473)
- [Sequence to Sequence Learning with Neural Networks](summaries/seq2seq_nn.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOQ1l5aUF4RWYtenc/view?usp=sharing), [arXiv](https://arxiv.org/abs/1409.3215)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](summaries/learning_phrase_rep_RNN_encoder_decoder_mt.md), [arXiv](https://arxiv.org/abs/1406.1078)
- [Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network](summaries/implicit_drd_grn.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOLUxtemFqejJmNVU/view?usp=sharing), [acl](https://www.aclweb.org/anthology/P/P16/P16-1163.pdf)

### 2016-10
- Learning Structured Output Representation using Deep Conditional Generative Models, Sohn et al 2015. (Conditional VAE) [nips](https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models), [blog/code](https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/), [code](https://github.com/hwalsuklee/tensorflow-mnist-CVAE)
- [Auto-Encoding Variational Bayes](summaries/auto-encoding_var_bayes.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOcjBIeVBZcTFUQ2s/view?usp=sharing), [arXiv](https://arxiv.org/abs/1312.6114), [blog/code](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/), - [Semi-supervised Variational Autoencoders for Sequence Classification](summaries/var_auto_sequence_class.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOTXEzLWlNQy1od0k/view?usp=sharing), [arXiv](https://arxiv.org/abs/1603.02514)
- [Autoencoder review](summaries/autoencoders.md) by Keras author Francois Chollet

## Datasets
- UCI [machine learning repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=&sort=instDown&view=table). 360 datasets, some very large. Nice sorting feature, such as ">1000 instance/classification/text" results in [14 data sets](https://archive.ics.uci.edu/ml/datasets.html?format=&task=cla&att=&area=&numAtt=&numIns=greater1000&type=&sort=instDown&view=table)

## Paper collections
- ["Awesome deep learning papers"]https://github.com/terryum/awesome-deep-learning-papers/), a collection of 100 best papers from past few years
- Paper collection by [songrotek](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap/blob/master/README.md)

## Overview
- [Nature Review article. Lecun, Bengio, Hinton. 2015](http://www.nature.com/articles/nature14539.epdf?referrer_access_token=K4awZz78b5Yn2_AoPV_4Y9RgN0jAjWel9jnR3ZoTv0PU8PImtLRceRBJ32CtadUBVOwHuxbf2QgphMCsA6eTOw64kccq9ihWSKdxZpGPn2fn3B_8bxaYh0svGFqgRLgaiyW6CBFAb3Fpm6GbL8a_TtQQDWKuhD1XKh_wxLReRpGbR_NdccoaiKP5xvzbV-x7b_7Y64ZSpqG6kmfwS6Q1rw%3D%3D&tracking_referrer=www.nature.com)
  * Good short overview
- [Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85â117.](http://arxiv.org/abs/1404.7828)
    * Extensive overview

## Neural Networks Basics

- [Michael Nielsen book on NN](http://neuralnetworksanddeeplearning.com/chap1.html)
- [Hacker's guide to Neural Networks. Andrej Karpathy blog](http://karpathy.github.io/neuralnets/)
- [Visualize NN training](http://experiments.mostafa.io/public/ffbpann/)

## Backpropagation

- [A Gentle Introduction to Backpropagation. Sathyanarayana (2014)](http://numericinsight.com/uploads/A_Gentle_Introduction_to_Backpropagation.pdf)
- [Learning representations by back-propagating errors. Hinton et al, 1986](http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html)
  * Seminal paper by Hinton et al on back-propagation.
- [The Backpropagation Algorithm](http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf)
  * Longer tutorial on the topic, 34 pages
- [Overview of various optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)
  * [Summary](summaries/overview_optimization.md)

## Misc
- Multi-Task Learning Objectives for Natural Language Processing, [blog](http://ruder.io/multi-task-learning-nlp/index.html)

## Recurrent Neural Network (RNN)

- [Blog intro, tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Cho et al. 2014)](http://arxiv.org/abs/1406.1078)
- [Character-Aware Neural Language Models. Kim et al. 2015.](http://arxiv.org/pdf/1508.06615.pdf)
- [The Unreasonable Effectiveness of Recurrent Neural Networks. Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
  * Indepth, examples in vision and NLP. Provides code
- [Sequence-to-Sequence Learning with Neural Networks. Sutskever et al (2014)](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
  * Ground-breaking work on machine translation with RNN and LSTM
- [Training RNN. Sutskever thesis. 2013](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)
  * Indepth, self-contained, 85 pages
- [Understanding Natural Language with Deep Neural Networks Using Torch (2015)](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)
  * See part on predicting next word with RNN.
- [LSTM BASED RNN ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION](http://arxiv.org/pdf/1402.1128v1.pdf)
- [Awesome Recurrent Neural Networks](https://github.com/kjw0612/awesome-rnn#lectures)
  * Curated list of RNN resources

## CNNs
- [Karpathy cs231 review](http://cs231n.github.io/convolutional-networks/)
- [Character-level Convolutional Networks for Text Classification](http://arxiv.org/abs/1509.01626)
  * [Annotated](https://drive.google.com/open?id=0ByV7wn2NzevOZEw4QV9tbFNyVTQ)
- [Collobert. Natural Language Processing (Almost) from Scratch (2011)](http://dl.acm.org/citation.cfm?id=2078186)
  * Spurred interest in applying CNN to NLP.
- [Multichannel Variable-Size Convolution for Sentence Classification. Yin, 2015](https://aclweb.org/anthology/K/K15/K15-1021.pdf)
  * Interesting, borrows multichannel from image CNN, where each channel is a different word embedding.
- [A CNN for Modelling Sentences. Kalchbrenner et al, 2014](http://phd.nal.co/papers/Kalchbrenner_DCNN_ACL14)
  * Dynamic k-max pooling for variable length sentences.
- [Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling. Xu et al, 2015](http://arxiv.org/pdf/1506.07650v1.pdf)
- [Text Understanding from Scratch. Zhang, LeCunn. (2015)](http://arxiv.org/abs/1502.01710)
- [Kim. Convolutional Neural Networks for Sentence Classification (2014)](http://arxiv.org/pdf/1408.5882v2.pdf)
- [Sensitivity Analysis of (And Practitioner's Guide to) CNN for Sentence Classification. Zhang, Wallace (2015)](http://arxiv.org/pdf/1510.03820v2.pdf)
  * [Annotated](https://drive.google.com/open?id=0ByV7wn2NzevOY25JNlJQREVLZEU)
- [Relation Extraction: Perspective from Convolutional Neural Networks. Nguyen, Grishman (2015)](http://www.cs.nyu.edu/~thien/pubs/vector15.pdf)
  * [Annotated](https://drive.google.com/file/d/0ByV7wn2NzevObzAtV1QyUDl5X2M/view?usp=sharing)
- [Convolutional Neural Network for Sentence Classification. Yahui Chen, 2015](https://uwspace.uwaterloo.ca/bitstream/handle/10012/9592/Chen_Yahui.pdf?sequence=3&isAllowed=y)
  * Master's thesis, University of Waterloo

## Deep Reinforcement Learning
- [Playing Atari with Deep Reinforcement Learning. Mnih et al. (2014)](http://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
- [Youtube Demo](https://www.youtube.com/watch?v=wfL4L_l4U9A)
- Simple Reinforcement Learning with TensorFlow series, part [0](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
- Basic DQN in Keras, [`blog`](https://keon.io/deep-q-learning/), [`code`](https://github.com/keon/deep-q-learning)
- Minimal and clean examples, [`code`](https://github.com/rlcode/reinforcement-learning)
- Demystifying Deep RL, [`blog`](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)
- Berkeley course on DRL, [`course`](http://rll.berkeley.edu/deeprlcourse/)

## Online Courses
- [Deep Learning. Udacity, 2015](https://www.udacity.com/course/deep-learning--ud730)
  * Very brief. It is more about getting a feel for DL and specifically about using TensorFlow for DL.
- [Convolutional Neural Networks for Visual Recognition. Stanford, 2016](http://cs231n.stanford.edu/)
- [Neural Network Course. UniversitÃ© de Sherbrooke, 2013](http://info.usherbrooke.ca/hlarochelle/neural_networks/description.html)
- [Machine Learning Course, University of Oxford(2014-2015)](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)
- [Deep Learning for NLP, Stanford (2015)](http://cs224d.stanford.edu/)
  * Click "syllabus" for full material
- [Stanford Deep Learning tutorials](http://ufldl.stanford.edu/tutorial/)
  * From basics of Machine Learning, to DNN, CNN, and others.
  * Includes code.

## Books
- [Ian Goodfellow, Yoshua Bengio, Aaron Courville (2016). Deep Learning.](http://www.deeplearningbook.org)
