### 2018-10
- Certified Defenses against Adversarial Examples, Raghunathan et al, 2018, [arXiv](https://arxiv.org/abs/1801.09344)
- Speaker-Follower Models for Vision-and-Language Navigation, Fried et al, NIPS 2018, [arXiv](https://arxiv.org/abs/1806.02724)
- Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies, Grusky et al, NAACL 2018, [arXiv](https://arxiv.org/abs/1804.11283)
- Architectural Complexity Measures of Recurrent Neural Networks, Zhang et al, NIPS 2016, [arXiv](https://arxiv.org/abs/1602.08210)
- Gradient Estimation Using Stochastic Computation Graphs, Schulman et al, 2016. [arXiv](https://arxiv.org/abs/1506.05254)
- Variational Inference: A Review for Statisticians, Blei et al, 2018. [arXiv](https://arxiv.org/abs/1601.00670)
- Variational Inference with Normalizing Flows, Rezende et al, 2016. [arXiv](https://arxiv.org/abs/1505.05770)
- Large Scale GAN Training for High Fidelity Natural Image Synthesis, Brock et al, submission to ICLR 2019. [arXiv](https://arxiv.org/abs/1809.11096)
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al, 2018. [arXiv](https://arxiv.org/pdf/1810.04805.pdf)
- The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables, Maddison et al, 2017. [arXiv](https://arxiv.org/abs/1611.00712). Note, the Concrete is equivalent to the Gumbel-Softmax.
-Categorical Reparameterization with Gumbel-Softmax, Jang et al, 2017. [arXiv](https://arxiv.org/abs/1611.01144). Note: Gumbel-Softmax is equivalent to the Concrete distribution.

### 2018-09
- Universal Transformers, Dehghani et al, 2018. [arXiv](https://arxiv.org/abs/1807.03819), [`google blog post`](https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html)
- Phrase-Based & Neural Unsupervised Machine Translation, Lample et al, EMNLP 2018. [arXiv](https://arxiv.org/abs/1804.07755)
- Hybrid Reward Architecture for Reinforcement Learning, Seijen et al, 2017. [arXiv](https://arxiv.org/abs/1706.04208).

### 2018-08
- Vehicle Communication Strategies for Simulated Highway Driving, Resnick et al, 2017, NIPS 2017 Workshop on Emergent Communication.
- Emergent Communication through Negotiation, Cao et al, NIPS 2017 Workshop on Emergent Communication.
- Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, Athalye et al, ICML 2018. [arXiv](https://arxiv.org/abs/1802.00420). Defeats 7 of 9 recently introduced adversarial defense methods. Won best paper at ICML.
- Meta-Gradient Reinforcement Learning, Xu et al 2018, [arXiv](https://arxiv.org/abs/1805.09801)

### 2018-07
- Proximal Policy Optimization Algorithms, Schulman et al, 2018. [arXiv](https://arxiv.org/abs/1707.06347), [`openai blog`](https://blog.openai.com/openai-baselines-ppo/), OpenAIFive [`blogpost`] which applies scaled up PPO on Dota2
- What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties, Conneau et al, ACL 2018. [arXiv](https://arxiv.org/abs/1805.01070). The authors go through 10 probing tasks to find out some of the things the embeddings capture, trained with various architectures.
- Style Transfer Through Back-Translation, Prabhumoye et al, ACL 2018. [arXiv](https://arxiv.org/abs/1804.09000)
- Hierarchical Neural Story Generation, Fan et al, ACL 2018. [arXiv](https://arxiv.org/abs/1805.04833). Generate a short story based on a "prompt", impressive results. Also has some cool tricks, like model fusion, a different type of attention, k=10 sampling, etc.
- Representation Learning for Grounded Spatial Reasoning, Janner et al, ACL 2018. [arXiv](https://arxiv.org/abs/1707.03938)
- Generating Sentences by Editing Prototypes, Guu et al, ACL 2018. [arXiv](https://arxiv.org/abs/1709.08878)
- A Stochastic Decoder for Neural Machine Translation, Schulz et al, ACL 2018. [arXiv](https://arxiv.org/abs/1805.10844)
- The Hitchhikerâ€™s Guide to Testing Statistical Significance in Natural Language Processing, Dror et al, ACL 2018. [aclweb](http://aclweb.org/anthology/P18-1128)
- Stock Movement Prediction from Tweets and Historical Prices, Xu and Cohen, ACL 2018. [pdf](http://aclweb.org/anthology/P18-1183)
- Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context, Khandelwal et al, ACL 2018. [arXiv](https://arxiv.org/abs/1805.04623)
- Backpropagating through Structured Argmax using a SPIGOT, Peng et al, ACL 2018. [arXiv](https://arxiv.org/abs/1805.04658)
- Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum, Levy et al, ACL 2018. [arXiv](https://arxiv.org/abs/1805.03716)

### 2018-06
- Self-Imitation Learning, Oh et al, 2018. [arXiv](https://arxiv.org/abs/1806.05635). Performs on-policy A2C update, and off-polic SIL, which samples positive experiences from a replay buffer and uses a form of AC. 
- Improving Language Understanding with Unsupervised Learning, Radford et al, 2018. [openai](https://blog.openai.com/language-unsupervised/)
- Prioritized Experience Replay, Schaul et al, ICLR 2016. [arXiv](https://arxiv.org/abs/1511.05952)
- Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation, Wu et al, 2017. [arXiv]()
- Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach, Karakida et al, 2018. [arXiv](https://arxiv.org/abs/1806.01316)
- On Learning Intrinsic Rewards for Policy Gradient Methods, Zheng et al, 2018. [arXiv](https://arxiv.org/abs/1804.06459)
- Breaking the Softmax Bottleneck: A High-Rank RNN Language Model, Yang et al, ICLR 2018. [openreview](https://openreview.net/forum?id=HkwZSG-CZ), [arXiv](https://arxiv.org/abs/1711.03953). [`summary`](summaries/softmax_bottleneck.md). Given a language model output matrix A over time, where each row is is the the vocabulary distribution given context, the authors hypothesize A  must be high rank to be express complex language, and the single softmax is not expressive enough. They propose a mixture of many softmax. 
- Measuring the Intrinsic Dimension of Objective Landscapes, Li et al, ICLR 2018. [openreview](), [arXiv](https://arxiv.org/abs/1804.08838), [`summary`](summaries/intrinsic_dimension.md). Intrinsic Dimension is the minimal parameter subspace (projected to the total parameters) to achieve a certain performance. It is a measure of model-problem complexity.
- Control of Memory, Active Perception, and Action in Minecraft, Oh et al, ICML 2016. [arXiv](https://arxiv.org/abs/1605.09128)
- Multitask Learning, Rich Caruana, PhD thesis 1997. [pdf](http://reports-archive.adm.cs.cmu.edu/anon/1997/CMU-CS-97-203.pdf). Work in the 90s on transfer learning! Chapter 5 discusses auxliary tasks for neural nets! 20 years before the UNREAL paper!
- Neural Map: Structured Memory for Deep Reinforcement Learning, Parisotto and Salakhutdinov, ICLR 2018. [arXiv](https://arxiv.org/abs/1702.08360). Instead of free external memory, have memory locations correlate with agent location, i.e. structured memory. Hugely outperforms memory nets and others on maze problems.
- On the State of the Art of Evaluation in Neural Language Models, ICLR 2018. [openreview](https://openreview.net/forum?id=ByJHuTgA-&). Some simple language models, like LSTM, actually achieve SOTA or near SOTA with proper hyperparams and simple additions, like shared embeddings and variational dropout (see Table 4 ablation).
- Reinforcement Learning with Unsupervised Auxiliary Tasks, Jaderberg et al, ICLR 2017. [openreview](https://openreview.net/forum?id=SJ6yPD5xg). Introduces the UNREAL model. See Caruana PhD thesis above from 1997, discusses auxiliary tasks for better representations!

### 2018-03
- Parameter Space Noise for Exploration, Plappert et al, ICLR 2018. [arXiv](https://arxiv.org/abs/1706.01905). Instead of adding noise to action space, add noise to the FA's parameters for better exploration.
- Continuous control with deep reinforcement learning, Lillicrap et al, ICLR 2016. [arXiv](https://arxiv.org/abs/1509.02971). Introduced Deep Deterministic Policy Gradient (DDPG), an actor critic algorithm applicable to continuous action spaces, off-policy.
- Deterministic Policy Gradient Algorithms, Silver et al, ICML 2014. [pdf](http://proceedings.mlr.press/v32/silver14.pdf). DPG is the expected gradient of the action-value function, easier to estimate than the traditional stochastic policy gradient.
- Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs, Murdoch et al, 2018, ICLR 2018. [pdf](https://openreview.net/pdf?id=rkRwGg-0Z), [arXiv](https://arxiv.org/abs/1801.05453)
- Emergence Of Linguistic Communication From Referential Games With Symbolic And Pixel Input, Lazaridou et al, ICLR 2018. [pdf](https://openreview.net/pdf?id=HJGv1Z-AW)
- Emergent Communication in a Multi-Modal, Multi-Step Referential Game, Evtimova et al, ICLR 2018. [arXiv](https://arxiv.org/abs/1705.10369), [`code`](https://github.com/nyu-dl/MultimodalGame/blob/master/model.py)
- Neural Speed Reading via Skim-RNN, Seo et al, ICLR 2018. [arXiv](https://arxiv.org/abs/1711.02085)
- Dynamic Word Embeddings for Evolving Semantic Discovery, Yao et al, 2017. [arXiv](https://arxiv.org/abs/1703.00607)

### 2018-02
- One Model To Learn Them All, Kaiser et al, 2017. [arXiv](https://arxiv.org/abs/1706.05137)
- An Analysis of Temporal-Difference Learning with Function Approximation, Tsitsiklis and Van Roy, 1997. [pdf](http://web.mit.edu/jnt/www/Papers/J063-97-bvr-td.pdf)
- Steps Toward Artificial Intelligence, Minsky, 1961. [pdf](https://courses.csail.mit.edu/6.803/pdf/steps.pdf)
- Eye on the Prize, Nilsson, 1995. [pdf](http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General%20Essays/AIMag16-02-002.pdf)
- The Option-Critic Architecture, Bacon et al. [arXiv](https://arxiv.org/abs/1609.05140)
- Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings. He et al, 2017. [arXiv](https://arxiv.org/abs/1704.07130)
- Learning to Win by Reading Manuals in a Monte-Carlo Framework, Branavan et al, 2012. [arXiv](https://arxiv.org/abs/1401.5390)

### 2017-12
- Generating Sentences by Editing Prototypes, Guu et al, 2017. [arXiv](https://arxiv.org/abs/1709.08878)
- SenGen: Sentence Generating Neural Variational Topic Model, Nallapati et al, 2017. [arXiv](https://arxiv.org/abs/1708.00308)
- Learning Sparse Neural Networks through L0 Regularization, Louizos et al 2017. [arXiv](https://arxiv.org/abs/1712.01312)
- Sparsity and the Lasso, Tibshirani and Wasserman, 2015. [pdf](http://www.stat.cmu.edu/~larry/=sml/sparsity.pdf). Note: related L0 paper above
- Proving convexity, Loh 2013. [pdf](http://www.math.cmu.edu/~ploh/docs/math/mop2013/convexity-soln.pdf). Note: related to L0 paper above
- Mathematics of Deep Learning, Vidal et al, 2017. [arXiv](https://arxiv.org/abs/1712.04741)
- Bayesian Hypernetworks, Krueger et al, 2017. [arXiv](https://arxiv.org/abs/1710.04759)
- SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents, Nallapati et al, 2016. [arXiv](https://arxiv.org/abs/1611.04230)
- Learning Online Alignments with Continuous Rewards Policy Gradient, Luo et al 2016. [arXiv](https://arxiv.org/abs/1608.01281)
- Asynchronous Methods for Deep Reinforcement Learning. Mnih et al, 2016. [arXiv](https://arxiv.org/abs/1602.01783). Introduces A3C, Asyncrhonous Advantage Actor Critic
- On The State of The Art In Neural Language Models, Anonymous, 2017. [iclr pdf](https://openreview.net/pdf?id=ByJHuTgA-)
- Natural Language Inference with External Knowledge, Chen et al 2017. [arXiv](https://arxiv.org/abs/1711.04289)

### 2017-11
- Memory Augmented Neural Networks with Wormhole Connections, Gulcehre et al, 2017. [arXiv](https://arxiv.org/abs/1701.08718)
- Emergence of Invariance and Disentangling in Deep Representations, Achille et al, 2017. [arXiv](https://arxiv.org/abs/1706.01350)
- Distilling the Knowledge in a Neural Network, Hinton et al, 2015. [arXiv](https://arxiv.org/abs/1503.02531)
- Seq2SQL: Generating Stuctured Queries From Natural Language Using Reinforcement Learning, Zhong et al, 2017. [arXiv](https://arxiv.org/abs/1709.00103)
- Better Text Understanding Through Image-To-Text Transfer, Kurach, 2017. [arXiv](
- Data Augmentation Generative Adversarial Networks, Antoniou et al, 2017. [arXiv](https://arxiv.org/abs/1711.04340)
- Adversarial Training Methods for Semi-Supervised Text Classification, Miyato et al, 2017. [arXiv](https://arxiv.org/abs/1605.07725)
- Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training, Anonymous, 2017. [openreview](https://openreview.net/pdf?id=SkhQHMW0W)
- Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling, Inan et al 2017. [arXiv](https://arxiv.org/abs/1611.01462)
- Building machines that learn and think for themselves, Botvinick et al, 2017. [cambridge](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-for-themselves/E28DBFEC380D4189FB7754B50066A96F)
- Neural Discrete Representation Learning, va den Oord et al, 2017. [arXiv](https://arxiv.org/abs/1711.00937)
- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets, Chen et al, 2016. [arXiv](https://arxiv.org/abs/1606.03657), [`blog`](https://towardsdatascience.com/infogan-generative-adversarial-networks-part-iii-380c0c6712cd), [`code`](https://github.com/zjost/InfoGAN)
- Evolution Strategies, Otoro 2017, blog part [1](http://blog.otoro.net/2017/10/29/visual-evolution-strategies/), [2](http://blog.otoro.net/2017/11/12/evolving-stable-strategies/)
- Matrix Capsules with EM Routing. Anonymous (likely Hinton lab), 2017. [openreview](https://openreview.net/pdf?id=HJWLfGWRb).
- Dynamic Routing Between Capsules, Sabour et al, 2017. [arXiv](https://arxiv.org/abs/1710.09829). [`code-keras`](https://github.com/XifengGuo/CapsNet-Keras), [`video review`](https://youtu.be/pPN8d0E3900)
- Weighted Transformer Network for Machine Translation, Ahmed et al, 2017. [arXiv](https://arxiv.org/abs/1711.02132)
- Unsupervised Machine Translation Using Monolingual Corpora Only, Lample et al, 2017. [arXiv](https://arxiv.org/abs/1711.00043)
- Non-Autoregressive Neural Machine Translation, Gu et al, 2017. [arXiv](https://arxiv.org/abs/1711.02281)
- Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments, Lowe et al, 2017. [arXiv](https://arxiv.org/abs/1706.02275)

### 2017-10
- Adversarial Learning for Neural Dialogue Generation, Li et al, 2017. [arXiv](https://arxiv.org/abs/1701.06547)
- Frustratingly Short Attention Spans in Neural Language Modeling, Daniluk et al, 2017. [arXiv](https://arxiv.org/abs/1702.04521)
- Adversarial Training Methods for Semi-Supervised Text Classification, Miyato et al, 2017. [arXiv](https://arxiv.org/abs/1605.07725)
- Progressive Growing of GANs for Improved Quality, Stability, and Variation, Karras et al, 2017. [pdf](http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of//karras2017gan-paper.pdf)
- A Closer Look at Memorization in Deep Networks, Arpit et al, 2017. [arXiv](https://arxiv.org/abs/1706.05394)
- Understanding deep learning requires rethinking generalization, Zhang et al, 2016. [arXiv](https://arxiv.org/abs/1611.03530)
- The Loss Surfaces of Multilayer Networks, Choromanska et al, 2015. [arXiv](https://arxiv.org/abs/1412.0233)
- Meta Learning Shared Hierarchies, Frans et al, 2017. [arXiv](https://arxiv.org/abs/1710.09767), [`author blog`](https://blog.openai.com/learning-a-hierarchy/)
- Mastering the game of Go without human knowledge, Silver et al, 2017. [arXiv](https://www.nature.com/nature/journal/v550/n7676/full/nature24270.html), [`blog`](http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/)
- Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation, Sharma et al, 2017. [arXiv](https://arxiv.org/abs/1706.09799)
- GuessWhat?! Visual object discovery through multi-modal dialogue, de Vries et al, 2017. [arXiv](https://arxiv.org/abs/1611.08481)
- A Frame Tracking Model for Memory-Enhanced Dialogue Systems, Schulz et al, 2017. [arXiv](https://arxiv.org/abs/1706.01690)
- A Deep Reinforced Model for Abstractive Summarization, Paulus et al, 2017. [arXiv](https://arxiv.org/abs/1705.04304), [`author blog`](https://einstein.ai/research/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)
- (about ROUGE score for summarization) ROUGE: A Package for Automatic Evaluation of Summaries, Chin-Yew Lin, 2004. [acl](http://anthology.aclweb.org/W/W04/W04-1013.pdf)
- Rainbow: Combining Improvements in Deep Reinforcement Learning, Hessel et al, 2017. [arXiv](https://arxiv.org/abs/1710.02298)
- Language Modeling with Gated Convolutional Networks, Dauphin et al, 2017, [arXiv](https://arxiv.org/abs/1612.08083)
- Convolutional Sequence to Sequence Learning, Gehring et al, 2017. [arXiv](https://arxiv.org/abs/1705.03122)
- Emergence of Grounded Compositional Language in Multi-Agent Populations, Mordatch and Abbeel, 2017. [arXiv](https://arxiv.org/abs/1703.04908), [`author blog`](https://blog.openai.com/learning-to-communicate/). Note: related to Kottur et al 2017.

### 2017-09
- Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog, Kottur et al, 2017. [arXiv](https://arxiv.org/abs/1706.08502), [`code`](https://github.com/batra-mlp-lab/lang-emerge)
- Opening the black box of Deep Neural Networks via Information, Schwartz-Ziv and Tishbly, 2017. [arXiv](https://arxiv.org/abs/1703.00810), [m-p review](https://blog.acolyer.org/2017/11/15/opening-the-black-box-of-deep-neural-networks-via-information-part-i/)
- End-to-end Neural Coreference Resolution, Lee et al, 2017. [arXiv](https://arxiv.org/abs/1707.07045)
- Deep Reinforcement Learning for Mention-Ranking Coreference Models, Clark et al, 2016. [arXiv](https://arxiv.org/abs/1609.08667)
- Oriented Response Networks, Zhou et al 2017. [arXiv](https://arxiv.org/abs/1701.01833)
- Training RNNs as Fast as CNNs, Lei et al, 2017. [arXiv](https://arxiv.org/abs/1709.02755)
- Quasi-Recurrent Neural Networks, Bradbury et al 2017. [arXiv](https://openreview.net/pdf?id=H1zJ-v5xl), [`author blog/code`](https://einstein.ai/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding)
- A Deep Reinforcement Learning Chatbot, Serban et al, 2017. [arXiv](https://arxiv.org/abs/1709.02349)
- Independently Controllable Factors, Thomas et al, 2017. [arXiv](https://arxiv.org/abs/1708.01289)
- Attention Is All You Need, Vaswani et al, 2017. [arXiv](https://arxiv.org/abs/1706.03762), [`code`](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py), [`google blog`](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html), [`reddit`](https://www.reddit.com/r/MachineLearning/comments/6gwqiw/r_170603762_attention_is_all_you_need_sota_nmt/)
- Attention-over-Attention Neural Networks for Reading Comprehension, Cui et al 2017. [arXiv](https://arxiv.org/abs/1607.04423), [`code`](https://github.com/OlavHN/attention-over-attention)
- Get To The Point: Summarization with Pointer-Generator Networks, See et al 2017. [arXiv](https://arxiv.org/abs/1704.04368), [`author blog`](http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html), [`code`](https://github.com/abisee/cnn-dailymail)
- Î²-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK, Higgins et al 2017. [pdf](https://openreview.net/pdf?id=Sy2fzU9gl)
- Massive Exploration of Neural Machine Translation Architectures, Britz et al 2017. [arXiv](https://arxiv.org/abs/1703.03906v2)
- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, 2017. [arXiv](https://arxiv.org/abs/1703.10593), ['examples'](https://junyanz.github.io/CycleGAN/), [`code-torch`](https://github.com/junyanz/CycleGAN), [`code-PyT`](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)

### 2017-08
- A Brief Survey of Deep Reinforcement Learning, Arulkumaran et al 2017. [arXiv](https://arxiv.org/abs/1708.05866)
- Regularizing and Optimizing LSTM Language Models, Merity et al 2017. [arXiv](http://lanl.arxiv.org/abs/1708.02182v1)
- Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets, Yang et al 2017. [arXiv](https://arxiv.org/abs/1703.04887)
- Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders, Zhao et al 2017. [arXiv](https://arxiv.org/abs/1703.10960)
- How to Train Your DRAGAN, Kodali et al 2017. [arXiv](https://arxiv.org/abs/1705.07215)
- Improved Training of Wasserstein GANs, Gulrajani et al 2017. [arXiv](https://arxiv.org/abs/1704.00028), [`blog`](https://lernapparat.de/improved-wasserstein-gan/), [`blog`](http://lernapparat.de/more-improved-wgan/), [`code`](https://github.com/igul222/improved_wgan_training)
- Wasserstein Gan, Arjovsky et al 2017. [arXiv](https://arxiv.org/abs/1701.07875), [`read-through`](http://www.alexirpan.com/2017/02/22/wasserstein-gan.html), [`Kantorovich-Rubinstein duality`](https://vincentherrmann.github.io/blog/wasserstein/), [`WGAN-tensorflow`](https://github.com/shekkizh/WassersteinGAN.tensorflow), [`blog/code`](https://wiseodd.github.io/techblog/2017/02/04/wasserstein-gan/)
- Reading Scene Text in Deep Convolutional Sequences, He et al, 2016. [arXiv](https://arxiv.org/abs/1506.04395)

### 2017-03
- Recurrent Batch Normalization, Cooijmans et al, 2017. [arXiv](https://arxiv.org/abs/1603.09025), [`code-tf`](https://github.com/OlavHN/bnlstm)
- An Actor-Critic Algorithm for Sequence Prediction, Bahdanau et al 2017. [arXiv](https://arxiv.org/abs/1607.07086), [`code`](https://github.com/rizar/actor-critic-public)
- Scheduled Sampling for Sequence Prediction with RNN, Bengio et al, 2015 [arXiv](https://arxiv.org/abs/1506.03099), [`summary`](summaries/scheduled_sampling.md),
- Hybrid computing using a neural network with dynamic external memory, published in [Nature](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf)
- Neural Turing Machine, [arXiv](https://arxiv.org/abs/1410.5401)
- LEARNING END-TO-END GOAL-ORIENTED DIALOG, Bordes et al, 2017. [arXiv](https://arxiv.org/abs/1605.07683), [`code`](https://github.com/carpedm20/MemN2N-tensorflow)
- End-To-End Memory Networks, Sukhbaatar et al, 2015, [arXiv](https://arxiv.org/abs/1503.08895)
- Memory Networks, [arXiv](https://arxiv.org/abs/1410.3916)
- Deep Photo Style Transfer, [arXiv](https://arxiv.org/abs/1703.07511)
- Matching Networks for One Shot Learning, Vinyals et al, NIPS 2016. [arXiv](https://arxiv.org/abs/1606.04080). [`summary`](summaries/matching_networks.md), [`code`](https://github.com/zergylord/oneshot). [`karpathy notes`](http://www.shortscience.org/paper?bibtexKey=journals/corr/VinyalsBLKW16#karpathy), [`Colyer blog`](https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/)

### 2017-01
- Optimization As A Model For Few-Shot Learning, Sachin Ravi and Hugo Larochelle, ICLR 2017. [openreview](https://openreview.net/pdf?id=rJY0-Kcll), [video](https://www.youtube.com/watch?v=igJmB6d8y8E)
- NIPS 2016 Tutorial:Generative Adversarial Networks, [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOa2RqZmJVR2hrUTA/view?usp=sharing),[arXiv](https://arxiv.org/abs/1701.00160), [blog/code](https://wiseodd.github.io/techblog/2016/09/17/gan-tensorflow/)

### 2016-11
- [Fully Character-Level Neural Machine Translation without Explicit Segmentation](summaries/fully_char_level_nmt.md), [annotated](https://drive.google.com/open?id=0ByV7wn2NzevOQ0JtTTRuR0pjUlE), [arXiv](https://arxiv.org/abs/1610.03017)
- [Neural Machine Translation by Jointly Learning to Align and Translate](summaries/neural_machine_translation.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOS3FmWHVNazhnczA/view?usp=sharing), [arXiv](https://arxiv.org/abs/1409.0473)
- [Sequence to Sequence Learning with Neural Networks](summaries/seq2seq_nn.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOQ1l5aUF4RWYtenc/view?usp=sharing), [arXiv](https://arxiv.org/abs/1409.3215)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](summaries/learning_phrase_rep_RNN_encoder_decoder_mt.md), [arXiv](https://arxiv.org/abs/1406.1078)
- [Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network](summaries/implicit_drd_grn.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOLUxtemFqejJmNVU/view?usp=sharing), [acl](https://www.aclweb.org/anthology/P/P16/P16-1163.pdf)

### 2016-10
- Learning Structured Output Representation using Deep Conditional Generative Models, Sohn et al 2015. (Conditional VAE) [nips](https://papers.nips.cc/paper/5775-learning-structured-output-representation-using-deep-conditional-generative-models), [blog/code](https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/), [code](https://github.com/hwalsuklee/tensorflow-mnist-CVAE)
- [Auto-Encoding Variational Bayes](summaries/auto-encoding_var_bayes.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOcjBIeVBZcTFUQ2s/view?usp=sharing), [arXiv](https://arxiv.org/abs/1312.6114), [blog/code](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/), - [Semi-supervised Variational Autoencoders for Sequence Classification](summaries/var_auto_sequence_class.md), [annotated](https://drive.google.com/file/d/0ByV7wn2NzevOTXEzLWlNQy1od0k/view?usp=sharing), [arXiv](https://arxiv.org/abs/1603.02514)
- [Autoencoder review](summaries/autoencoders.md) by Keras author Francois Chollet

## Datasets
- UCI [machine learning repository](https://archive.ics.uci.edu/ml/datasets.html?format=&task=&att=&area=&numAtt=&numIns=&type=&sort=instDown&view=table). 360 datasets, some very large. Nice sorting feature, such as ">1000 instance/classification/text" results in [14 data sets](https://archive.ics.uci.edu/ml/datasets.html?format=&task=cla&att=&area=&numAtt=&numIns=greater1000&type=&sort=instDown&view=table)

## Paper collections
- ["Awesome deep learning papers"]https://github.com/terryum/awesome-deep-learning-papers/), a collection of 100 best papers from past few years
- Paper collection by [songrotek](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap/blob/master/README.md)

## Overview
- [Nature Review article. Lecun, Bengio, Hinton. 2015](http://www.nature.com/articles/nature14539.epdf?referrer_access_token=K4awZz78b5Yn2_AoPV_4Y9RgN0jAjWel9jnR3ZoTv0PU8PImtLRceRBJ32CtadUBVOwHuxbf2QgphMCsA6eTOw64kccq9ihWSKdxZpGPn2fn3B_8bxaYh0svGFqgRLgaiyW6CBFAb3Fpm6GbL8a_TtQQDWKuhD1XKh_wxLReRpGbR_NdccoaiKP5xvzbV-x7b_7Y64ZSpqG6kmfwS6Q1rw%3D%3D&tracking_referrer=www.nature.com)
  * Good short overview
- [Schmidhuber, J. (2015). Deep learning in neural networks: An overview. Neural Networks, 61, 85â€“117.](http://arxiv.org/abs/1404.7828)
    * Extensive overview

## Neural Networks Basics

- [Michael Nielsen book on NN](http://neuralnetworksanddeeplearning.com/chap1.html)
- [Hacker's guide to Neural Networks. Andrej Karpathy blog](http://karpathy.github.io/neuralnets/)
- [Visualize NN training](http://experiments.mostafa.io/public/ffbpann/)

## Backpropagation

- [A Gentle Introduction to Backpropagation. Sathyanarayana (2014)](http://numericinsight.com/uploads/A_Gentle_Introduction_to_Backpropagation.pdf)
- [Learning representations by back-propagating errors. Hinton et al, 1986](http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html)
  * Seminal paper by Hinton et al on back-propagation.
- [The Backpropagation Algorithm](http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf)
  * Longer tutorial on the topic, 34 pages
- [Overview of various optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)
  * [Summary](summaries/overview_optimization.md)

## Misc
- Multi-Task Learning Objectives for Natural Language Processing, [blog](http://ruder.io/multi-task-learning-nlp/index.html)

## Recurrent Neural Network (RNN)

- [Blog intro, tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Cho et al. 2014)](http://arxiv.org/abs/1406.1078)
- [Character-Aware Neural Language Models. Kim et al. 2015.](http://arxiv.org/pdf/1508.06615.pdf)
- [The Unreasonable Effectiveness of Recurrent Neural Networks. Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
  * Indepth, examples in vision and NLP. Provides code
- [Sequence-to-Sequence Learning with Neural Networks. Sutskever et al (2014)](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
  * Ground-breaking work on machine translation with RNN and LSTM
- [Training RNN. Sutskever thesis. 2013](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)
  * Indepth, self-contained, 85 pages
- [Understanding Natural Language with Deep Neural Networks Using Torch (2015)](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)
  * See part on predicting next word with RNN.
- [LSTM BASED RNN ARCHITECTURES FOR LARGE VOCABULARY SPEECH RECOGNITION](http://arxiv.org/pdf/1402.1128v1.pdf)
- [Awesome Recurrent Neural Networks](https://github.com/kjw0612/awesome-rnn#lectures)
  * Curated list of RNN resources

## CNNs
- [Karpathy cs231 review](http://cs231n.github.io/convolutional-networks/)
- [Character-level Convolutional Networks for Text Classification](http://arxiv.org/abs/1509.01626)
  * [Annotated](https://drive.google.com/open?id=0ByV7wn2NzevOZEw4QV9tbFNyVTQ)
- [Collobert. Natural Language Processing (Almost) from Scratch (2011)](http://dl.acm.org/citation.cfm?id=2078186)
  * Spurred interest in applying CNN to NLP.
- [Multichannel Variable-Size Convolution for Sentence Classification. Yin, 2015](https://aclweb.org/anthology/K/K15/K15-1021.pdf)
  * Interesting, borrows multichannel from image CNN, where each channel is a different word embedding.
- [A CNN for Modelling Sentences. Kalchbrenner et al, 2014](http://phd.nal.co/papers/Kalchbrenner_DCNN_ACL14)
  * Dynamic k-max pooling for variable length sentences.
- [Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling. Xu et al, 2015](http://arxiv.org/pdf/1506.07650v1.pdf)
- [Text Understanding from Scratch. Zhang, LeCunn. (2015)](http://arxiv.org/abs/1502.01710)
- [Kim. Convolutional Neural Networks for Sentence Classification (2014)](http://arxiv.org/pdf/1408.5882v2.pdf)
- [Sensitivity Analysis of (And Practitioner's Guide to) CNN for Sentence Classification. Zhang, Wallace (2015)](http://arxiv.org/pdf/1510.03820v2.pdf)
  * [Annotated](https://drive.google.com/open?id=0ByV7wn2NzevOY25JNlJQREVLZEU)
- [Relation Extraction: Perspective from Convolutional Neural Networks. Nguyen, Grishman (2015)](http://www.cs.nyu.edu/~thien/pubs/vector15.pdf)
  * [Annotated](https://drive.google.com/file/d/0ByV7wn2NzevObzAtV1QyUDl5X2M/view?usp=sharing)
- [Convolutional Neural Network for Sentence Classification. Yahui Chen, 2015](https://uwspace.uwaterloo.ca/bitstream/handle/10012/9592/Chen_Yahui.pdf?sequence=3&isAllowed=y)
  * Master's thesis, University of Waterloo

## Deep Reinforcement Learning
- [Playing Atari with Deep Reinforcement Learning. Mnih et al. (2014)](http://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
- [Youtube Demo](https://www.youtube.com/watch?v=wfL4L_l4U9A)
- Simple Reinforcement Learning with TensorFlow series, part [0](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
- Basic DQN in Keras, [`blog`](https://keon.io/deep-q-learning/), [`code`](https://github.com/keon/deep-q-learning)
- Minimal and clean examples, [`code`](https://github.com/rlcode/reinforcement-learning)
- Demystifying Deep RL, [`blog`](http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)
- Berkeley course on DRL, [`course`](http://rll.berkeley.edu/deeprlcourse/)

## Online Courses
- [Deep Learning. Udacity, 2015](https://www.udacity.com/course/deep-learning--ud730)
  * Very brief. It is more about getting a feel for DL and specifically about using TensorFlow for DL.
- [Convolutional Neural Networks for Visual Recognition. Stanford, 2016](http://cs231n.stanford.edu/)
- [Neural Network Course. UniversitÃ© de Sherbrooke, 2013](http://info.usherbrooke.ca/hlarochelle/neural_networks/description.html)
- [Machine Learning Course, University of Oxford(2014-2015)](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)
- [Deep Learning for NLP, Stanford (2015)](http://cs224d.stanford.edu/)
  * Click "syllabus" for full material
- [Stanford Deep Learning tutorials](http://ufldl.stanford.edu/tutorial/)
  * From basics of Machine Learning, to DNN, CNN, and others.
  * Includes code.

## Books
- [Ian Goodfellow, Yoshua Bengio, Aaron Courville (2016). Deep Learning.](http://www.deeplearningbook.org)
